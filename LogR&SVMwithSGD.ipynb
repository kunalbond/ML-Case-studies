{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "f1    0.067172\n",
       "f2   -0.017944\n",
       "f3    0.839060\n",
       "y     1.000000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "data = pd.read_csv('task_b.csv')\n",
    "data=data.iloc[:,1:]\n",
    "x = data.iloc[:,:-1]\n",
    "y= data.iloc[:,-1]\n",
    "data.corr()['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 10.78, NNZs: 3, Bias: -0.017500, T: 200, Avg. loss: 25162.588749\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.13, NNZs: 3, Bias: -0.015500, T: 400, Avg. loss: 26216.210376\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.49, NNZs: 3, Bias: -0.018500, T: 600, Avg. loss: 32851.729308\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.92, NNZs: 3, Bias: -0.027500, T: 800, Avg. loss: 30789.148650\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.76, NNZs: 3, Bias: -0.032500, T: 1000, Avg. loss: 30015.982607\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 9.92, NNZs: 3, Bias: -0.024500, T: 1200, Avg. loss: 29690.931055\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 6 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4.1639715 , -8.91579066,  1.28733856]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(eta0=0.001, alpha=0.001, loss='log',random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf.fit(X=x, y=y)\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "1. Considering the features are not multicollinear, we can directly use absolute value of weights and decide upon the most important features. If there is collinearity then we have to use other techniques like forward feature selection.\n",
    "2. Feature importance order: F2>F1>F3 according to weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6.11, NNZs: 3, Bias: -0.016000, T: 200, Avg. loss: 26336.457910\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.75, NNZs: 3, Bias: -0.011000, T: 400, Avg. loss: 25927.053650\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.56, NNZs: 3, Bias: -0.009000, T: 600, Avg. loss: 33077.171384\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.69, NNZs: 3, Bias: -0.027000, T: 800, Avg. loss: 31546.551215\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.30, NNZs: 3, Bias: -0.028000, T: 1000, Avg. loss: 30800.517488\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.27, NNZs: 3, Bias: -0.027000, T: 1200, Avg. loss: 30114.441106\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 6.93, NNZs: 3, Bias: -0.022000, T: 1400, Avg. loss: 30016.955979\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.81782233, -5.57723074,  1.53976662]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.SGDClassifier(eta0=0.001, alpha=0.001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf.fit(X=x, y=y)\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "1. Considering the features are not multicollinear, we can directly use the absolute value of weights and decide upon the most important features. If there is collinearity then we have to use other techniques like forward feature selection.\n",
    "2. Feature importance order: F2>F1>F3 according to weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: -0.001770, T: 200, Avg. loss: 0.677171\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.16, NNZs: 3, Bias: -0.006515, T: 400, Avg. loss: 0.646162\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.22, NNZs: 3, Bias: -0.013955, T: 600, Avg. loss: 0.621043\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.28, NNZs: 3, Bias: -0.023568, T: 800, Avg. loss: 0.600506\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.34, NNZs: 3, Bias: -0.034991, T: 1000, Avg. loss: 0.583303\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.047705, T: 1200, Avg. loss: 0.568775\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.44, NNZs: 3, Bias: -0.061532, T: 1400, Avg. loss: 0.556074\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.48, NNZs: 3, Bias: -0.076227, T: 1600, Avg. loss: 0.544991\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.091307, T: 1800, Avg. loss: 0.535102\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.107080, T: 2000, Avg. loss: 0.526230\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.59, NNZs: 3, Bias: -0.123032, T: 2200, Avg. loss: 0.518271\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.63, NNZs: 3, Bias: -0.139373, T: 2400, Avg. loss: 0.511006\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.66, NNZs: 3, Bias: -0.156119, T: 2600, Avg. loss: 0.504348\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.172665, T: 2800, Avg. loss: 0.498177\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.72, NNZs: 3, Bias: -0.189410, T: 3000, Avg. loss: 0.492509\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.74, NNZs: 3, Bias: -0.206093, T: 3200, Avg. loss: 0.487135\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.222830, T: 3400, Avg. loss: 0.482238\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.239751, T: 3600, Avg. loss: 0.477606\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.82, NNZs: 3, Bias: -0.256484, T: 3800, Avg. loss: 0.473213\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.84, NNZs: 3, Bias: -0.273092, T: 4000, Avg. loss: 0.469131\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.86, NNZs: 3, Bias: -0.289424, T: 4200, Avg. loss: 0.465228\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.88, NNZs: 3, Bias: -0.305765, T: 4400, Avg. loss: 0.461528\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.90, NNZs: 3, Bias: -0.321840, T: 4600, Avg. loss: 0.458015\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.92, NNZs: 3, Bias: -0.337956, T: 4800, Avg. loss: 0.454734\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.94, NNZs: 3, Bias: -0.353938, T: 5000, Avg. loss: 0.451552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.96, NNZs: 3, Bias: -0.369601, T: 5200, Avg. loss: 0.448527\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.98, NNZs: 3, Bias: -0.385183, T: 5400, Avg. loss: 0.445625\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.00, NNZs: 3, Bias: -0.400454, T: 5600, Avg. loss: 0.442911\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.01, NNZs: 3, Bias: -0.415628, T: 5800, Avg. loss: 0.440274\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.03, NNZs: 3, Bias: -0.430942, T: 6000, Avg. loss: 0.437699\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.04, NNZs: 3, Bias: -0.445748, T: 6200, Avg. loss: 0.435326\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.06, NNZs: 3, Bias: -0.460236, T: 6400, Avg. loss: 0.432998\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.08, NNZs: 3, Bias: -0.474568, T: 6600, Avg. loss: 0.430747\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.09, NNZs: 3, Bias: -0.488640, T: 6800, Avg. loss: 0.428569\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.10, NNZs: 3, Bias: -0.502506, T: 7000, Avg. loss: 0.426505\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.12, NNZs: 3, Bias: -0.516541, T: 7200, Avg. loss: 0.424531\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.13, NNZs: 3, Bias: -0.530200, T: 7400, Avg. loss: 0.422598\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.15, NNZs: 3, Bias: -0.543705, T: 7600, Avg. loss: 0.420787\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.16, NNZs: 3, Bias: -0.556845, T: 7800, Avg. loss: 0.419002\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.17, NNZs: 3, Bias: -0.569737, T: 8000, Avg. loss: 0.417244\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1.18, NNZs: 3, Bias: -0.582680, T: 8200, Avg. loss: 0.415617\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.20, NNZs: 3, Bias: -0.595494, T: 8400, Avg. loss: 0.413978\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.21, NNZs: 3, Bias: -0.607811, T: 8600, Avg. loss: 0.412415\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.22, NNZs: 3, Bias: -0.620238, T: 8800, Avg. loss: 0.410976\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.23, NNZs: 3, Bias: -0.632322, T: 9000, Avg. loss: 0.409560\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.24, NNZs: 3, Bias: -0.644198, T: 9200, Avg. loss: 0.408138\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.25, NNZs: 3, Bias: -0.656147, T: 9400, Avg. loss: 0.406800\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.26, NNZs: 3, Bias: -0.667777, T: 9600, Avg. loss: 0.405443\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.28, NNZs: 3, Bias: -0.679288, T: 9800, Avg. loss: 0.404254\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.29, NNZs: 3, Bias: -0.690669, T: 10000, Avg. loss: 0.402996\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 1.30, NNZs: 3, Bias: -0.701794, T: 10200, Avg. loss: 0.401811\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 1.31, NNZs: 3, Bias: -0.712715, T: 10400, Avg. loss: 0.400676\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 1.32, NNZs: 3, Bias: -0.723578, T: 10600, Avg. loss: 0.399571\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 1.33, NNZs: 3, Bias: -0.734451, T: 10800, Avg. loss: 0.398476\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 1.34, NNZs: 3, Bias: -0.744881, T: 11000, Avg. loss: 0.397441\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 1.35, NNZs: 3, Bias: -0.755182, T: 11200, Avg. loss: 0.396431\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 1.35, NNZs: 3, Bias: -0.765316, T: 11400, Avg. loss: 0.395447\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 1.36, NNZs: 3, Bias: -0.775259, T: 11600, Avg. loss: 0.394471\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 1.37, NNZs: 3, Bias: -0.785140, T: 11800, Avg. loss: 0.393555\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 1.38, NNZs: 3, Bias: -0.794991, T: 12000, Avg. loss: 0.392632\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 1.39, NNZs: 3, Bias: -0.804572, T: 12200, Avg. loss: 0.391753\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 61 epochs took 0.02 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.19809769, -0.1842337 ,  1.36427005]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1 = np.array(data.iloc[:,0])\n",
    "col2 = np.array(data.iloc[:,1])\n",
    "col3 = np.array(data.iloc[:,2])\n",
    "for i in range(len(col1)):\n",
    "    col1[i] = (col1[i]-col1.mean())/col1.std()\n",
    "    col2[i] = (col2[i]-col2.mean())/col2.std()\n",
    "    col3[i] = (col3[i]-col3.mean())/col3.std()\n",
    "data1 = {'f1':col1,'f2':col2,'f3':col3}\n",
    "df = pd.DataFrame(data1)\n",
    "clf = linear_model.SGDClassifier(eta0=0.001, alpha=0.001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf.fit(X=df, y=y)\n",
    "clf.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. Considering the features are not multicollinear, we can directly use absolute value of weights and decide upon the most important features. If there is collinearity then we have to use other techniques like forward feature selection.\n",
    "2. Feature importance order: F3>F1>F2 according to weights. We can see the same pattern in correlation also. F3 is strongly correlated than other features.  More weight is given to F3. Then F1 is strongly coorelated as compared to F2. More weight is given to F1 than F2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.18, NNZs: 3, Bias: 0.000000, T: 200, Avg. loss: 0.930971\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.34, NNZs: 3, Bias: -0.002000, T: 400, Avg. loss: 0.782287\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.45, NNZs: 3, Bias: -0.024000, T: 600, Avg. loss: 0.681001\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.53, NNZs: 3, Bias: -0.057000, T: 800, Avg. loss: 0.631480\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.095000, T: 1000, Avg. loss: 0.594566\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.66, NNZs: 3, Bias: -0.133000, T: 1200, Avg. loss: 0.566353\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.71, NNZs: 3, Bias: -0.173000, T: 1400, Avg. loss: 0.542882\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.75, NNZs: 3, Bias: -0.214000, T: 1600, Avg. loss: 0.524276\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.256000, T: 1800, Avg. loss: 0.506512\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.82, NNZs: 3, Bias: -0.298000, T: 2000, Avg. loss: 0.491116\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.338000, T: 2200, Avg. loss: 0.478464\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.87, NNZs: 3, Bias: -0.378000, T: 2400, Avg. loss: 0.465955\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.89, NNZs: 3, Bias: -0.409000, T: 2600, Avg. loss: 0.457045\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.91, NNZs: 3, Bias: -0.439000, T: 2800, Avg. loss: 0.449275\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.94, NNZs: 3, Bias: -0.469000, T: 3000, Avg. loss: 0.443229\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.96, NNZs: 3, Bias: -0.495000, T: 3200, Avg. loss: 0.436356\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.98, NNZs: 3, Bias: -0.523000, T: 3400, Avg. loss: 0.431474\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.99, NNZs: 3, Bias: -0.549000, T: 3600, Avg. loss: 0.426521\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.01, NNZs: 3, Bias: -0.572000, T: 3800, Avg. loss: 0.421538\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.02, NNZs: 3, Bias: -0.596000, T: 4000, Avg. loss: 0.417966\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.04, NNZs: 3, Bias: -0.619000, T: 4200, Avg. loss: 0.414211\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.05, NNZs: 3, Bias: -0.641000, T: 4400, Avg. loss: 0.410404\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.07, NNZs: 3, Bias: -0.658000, T: 4600, Avg. loss: 0.406622\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.08, NNZs: 3, Bias: -0.676000, T: 4800, Avg. loss: 0.403870\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.10, NNZs: 3, Bias: -0.695000, T: 5000, Avg. loss: 0.401063\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.11, NNZs: 3, Bias: -0.712000, T: 5200, Avg. loss: 0.398732\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.12, NNZs: 3, Bias: -0.727000, T: 5400, Avg. loss: 0.396258\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.13, NNZs: 3, Bias: -0.744000, T: 5600, Avg. loss: 0.394388\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.15, NNZs: 3, Bias: -0.759000, T: 5800, Avg. loss: 0.392514\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.16, NNZs: 3, Bias: -0.774000, T: 6000, Avg. loss: 0.390693\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.17, NNZs: 3, Bias: -0.789000, T: 6200, Avg. loss: 0.388886\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.18, NNZs: 3, Bias: -0.802000, T: 6400, Avg. loss: 0.387318\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.19, NNZs: 3, Bias: -0.813000, T: 6600, Avg. loss: 0.385646\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.19, NNZs: 3, Bias: -0.822000, T: 6800, Avg. loss: 0.384805\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.21, NNZs: 3, Bias: -0.829000, T: 7000, Avg. loss: 0.383669\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.21, NNZs: 3, Bias: -0.839000, T: 7200, Avg. loss: 0.383457\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.22, NNZs: 3, Bias: -0.848000, T: 7400, Avg. loss: 0.382447\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.23, NNZs: 3, Bias: -0.856000, T: 7600, Avg. loss: 0.381552\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.24, NNZs: 3, Bias: -0.865000, T: 7800, Avg. loss: 0.381087\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.25, NNZs: 3, Bias: -0.873000, T: 8000, Avg. loss: 0.380311\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1.25, NNZs: 3, Bias: -0.881000, T: 8200, Avg. loss: 0.379472\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.26, NNZs: 3, Bias: -0.886000, T: 8400, Avg. loss: 0.378256\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.27, NNZs: 3, Bias: -0.895000, T: 8600, Avg. loss: 0.378876\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.28, NNZs: 3, Bias: -0.903000, T: 8800, Avg. loss: 0.377553\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.29, NNZs: 3, Bias: -0.909000, T: 9000, Avg. loss: 0.377088\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.29, NNZs: 3, Bias: -0.918000, T: 9200, Avg. loss: 0.376543\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.30, NNZs: 3, Bias: -0.926000, T: 9400, Avg. loss: 0.376372\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 47 epochs took 0.02 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.168297  , -0.1116561 ,  1.28187845]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.SGDClassifier(eta0=0.001, alpha=0.001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf.fit(X=df, y=y)\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. Feature importance order: F3>F1>F2 according to weights. We can see the same pattern in correlation also. F3 is strongly correlated than other features. More weight is given to F3. Then F1 is strongly correlated as compared to F2. More weight is given to F1 than F2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
